\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading tackling_climate_workshop_style

% ready for submission
% \usepackage{tackling_climate_workshop_style}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
    \usepackage[preprint]{tackling_climate_workshop_style}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{tackling_climate_workshop_style}

% to avoid loading the natbib package, add option nonatbib:
     % \usepackage[nonatbib]{tackling_climate_workshop_style}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{amsmath}

\title{Interactive Atmospheric Composition Emulation for Next-Generation Earth System Models}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
    Mohammad H. Erfani\thanks{Corresponding author (\texttt{se2639@columbia.edu})} \\
    Center for Climate Systems Research\\
    Columbia University\\
    New York, NY 10025 \\
    % \\
    \And
    Kara D. Lamb \\
    Department of Earth and Environmental Engineering \\
    Columbia University \\
    New York, NY 10027\\
    % \texttt{kl3231@columbia.edu} \\
    \And
    Susanne E. Bauer \\
    NASA Goddard Institute for Space Studies\\
    New York, NY 10025 \\
    % \texttt{susanne.e.bauer@nasa.gov} \\
    \And
    Kostas Tsigaridis \\
    Center for Climate Systems Research\\
    Columbia University\\
    New York, NY 10025 \\
    % \texttt{kt2347@columbia.edu} \\
    \And
    Marcus van Lier-Walqui\\
    Center for Climate Systems Research\\
    Columbia University\\
    New York, NY 10025\\
    % \texttt{mv2525@columbia.edu} \\
    \And
    Gavin Schmidt\\
    NASA Goddard Institute for Space Studies\\
    New York, NY 10025 \\
    % \texttt{gavin.a.schmidt@nasa.gov} \\
}

\begin{document}

\maketitle

\begin{abstract}
    Interactive composition simulations are computationally expensive in Earth System Models (ESMs) because they transport numerous gaseous and aerosol tracers each time step. This limits higher-resolution transient climate simulations with current computational resources. ESMs like NASA GISS-E3 often use pre-computed monthly-averaged atmospheric composition concentrations (Non-Interactive Tracers or NINT) to reduce computational costs. While NINT significantly cuts computational demands, it fails to capture real-time feedback between aerosols and other climate processes by relying on pre-calculated fields. We extended the GISS ModelE NINT version using machine learning (ML) to create ``Smart NINT,'' which emulates interactive emissions. Smart NINT interactively calculates concentrations using ML with surface emissions and meteorological data as inputs, avoiding full physics parameterizations. Our approach utilizes spatiotemporal architecture that possess well-matched inductive bias to effectively capture the spatial and temporal dependencies in tracer evolution. Input data include prognostic/diagnostic quantities for horizontal advection, vertical convection, and sink/source terms, processed through the first 20 vertical levels (up to 600 hPa) using ModelE OMA scheme. This vertical range covers nearly the entire BCB concentration distribution in the troposphere. Our evaluation shows excellent model performance with R$^2$ values exceeding 0.93 and Pearson correlation coefficients above 0.97 at the first pressure level. This high performance continues through level 15 (800 hPa), then gradually decreases as BCB concentrations drop significantly. The model maintains acceptable performance even when tested on data from entirely different time periods outside the training domain, which is a crucial capability for climate modeling applications requiring reliable long-term projections. These results confirm that our approach successfully shifts the paradigm from simple numerical solver mimicry to spatio-temporal modeling, offering significant improvements in forecasting capability for long-term climate simulations.
\end{abstract}

\section{Introduction}
    Earth System Models (ESMs) are foundational tools in climate science, combining numerical representations of atmospheric fluid dynamics and other Earth physical processes—such as the ocean, land surface, and cryosphere, on three-dimensional grids [cite]. Despite their importance, ESMs still have significant shortcomings, including imperfect physical parameterizations, insufficient resolution to resolve fine-scale weather phenomena, and prohibitively high computational costs for operational forecasting [cite]. To address these limitations, researchers have increasingly turned to advanced machine learning (ML) techniques. These efforts were pioneered by [cite], building upon foundational studies such as [cites]. Deep Learning Weather Prediction (DLWP) models utilize deep convolutional neural networks (CNNs) for globally gridded weather prediction. The focus of this work is on developing a data-driven ML-based model that can be iteratively stepped forward, similar to traditional numerical weather prediction (NWP) models, to simulate atmospheric states at arbitrarily long lead times [cite]. Specifically, DLWP CNNs directly map atmospheric state $u(t)$ to its future state $u(t + \Delta t)$ by learning from historical weather observations, with $\Delta t$ set to 6 hours.

    Following works have maintained the paradigm of simple numerical solver mimicry while focusing on improving inductive biases for spatial dependencies and better representing grid data on the globe using equiangular gnomonic cubed spheres, spherical harmonic functions, or multi-mesh graph representations. These studies have also worked on improving and adopting more sophisticated ML modules such as Fourier neural operators, graph neural networks, and vision transformers. However, they still adopt autoregressive rollout approaches similar to traditional NWP systems—that is, they can be "rolled out" by feeding their own predictions back as input to generate arbitrarily long trajectories of weather states. In these methods, the next state is calculated solely from the current state, which represents the only mechanism by which time dependency is incorporated into the prediction process. This temporal dependency modeling appears insufficient, representing a critical gap since atmospheric dynamics are fundamentally time-dependent. The lack of components designed to capture trends, seasonality, and cyclical patterns significantly limits forecasting capability.
    
    In autoregressive approaches, even within the spatial domain, inductive biases are only considered along the horizontal dimension. In the vertical direction, all data are simply stacked along the channel dimension and fed into the model as input. In other words, the models receive an array of input values (prognostic/diagnostic variables) per horizontal grid cell and then process these in a latent space, apparently to account for vertical mixing and interactions across variables. However, the history of ML demonstrates that whenever a module is developed that introduces a strong inductive bias to naturally simulate the behavior of its corresponding natural phenomenon, it revolutionizes that area of expertise. This includes how the inherent structure and characteristics of CNNs aligned with visual recognition tasks revolutionized image classification and semantic segmentation in image processing, how attention mechanisms strongly simulate the nonlinear behavior of words in phrases and sentences and revolutionized natural language processing, or how probabilistic latent spaces in generative adversarial networks and diffusion models enabled the production of diverse visual content through learned distributional representations.
    
    We propose spatiotemporal ML as a substitute for the autoregressive rollout approach to capture the inherent high spatial and temporal dependencies in the data. The temporal module of these models, regardless of type, generally introduces a strong inductive bias for memorizing both short-term and long-term temporal dependencies. In addition to this, we introduce a module that includes spatial encoders to process two different modalities (emission and forcing) in 2D and 3D and transform them into a uniform dimensional latent space, eventually fusing them via Spatial Feature Transform (SFT). Since the horizontal dimension is preserved in this module, the output includes four dimensions—time, features, latitude, and longitude—and can easily be fed into any type of spatiotemporal model with different approaches for representing the spatial domain. We simply use ConvLSTM with a rectangular latitude-longitude grid representation, as previous studies have invested significantly in this area and it was not our primary concern in this study.

\section{Dataset Description}
    To train the ML model, the NASA GISS-E3 (ModelE) was run with prescribed sea surface temperature (SST) and sea ice fraction for three different time periods. First, pre industrial era between 1850 to 1853, excluding the first year as spin-up period entire 3 years (E3OMA1850M). For this period monthly-averaged BCB concentrations were interpolated on daily interval. this is becasue there is no daily inventory for those period of time. the second dataset falls between 2010 to 2012, 3 years, (E3OMA2010D). as this period falls into sattelite era we used daily BCB concentration for this run. the third dataset is 2020 and 2021 (E3OMA2020D), as 2010 dataset this run is based on daily BCB concentreation.  which is considered for testing the models on entirely different time periods outside the training domain.
    
    The atmospheric component of ModelE3 is set for this study to a horizontal resolution of 2$^\circ$ latitude by 2.5$^\circ$ longitude, with 62 vertical layers. The model's temporal resolution is set to half-hour intervals. Model outputs comprise of prognostic/diagnostics required to consider all sub-processes in BCB transport model including horizontal advection, vertical convection, and sink/source terms. The list of candidate variables, their description, units and dimensions on spatial domain are showed in table 1.
    
    \begin{table}[htbp]
        \begin{tabular}{llll}
            \hline
            Variable Name       & Description                       & Unit                               & Dim \\ \hline
            axyp                & Gridcell area                     & $\text{m}^2$                       & 2D  \\
            landfr              & Land fraction                     & \%                                 & 2D  \\
            prsurf              & Surface pressure                  & hPa (mb)                           & 2D  \\
            pblht$_{\text{bp}}$ & Planetary boundary layer          & m                                  & 2D  \\
            shflx               & Sensible heat flux                & $\text{W}/\text{m}^2$              & 2D  \\
            lhflx               & Latent heat flux                  & $\text{W}/\text{m}^3$              & 2D  \\
            BCB$_{\text{src}}$  & BCB source                        & $10^{-12}$ kg m$^{-2}$ s$^{-1}$    & 2D  \\
            u                   & East-west velocity                & m/s                                & 3D  \\
            v                   & North-south velocity              & m/s                                & 3D  \\
            omega               & Pressure vertical velocity        & hPa/s                              & 3D  \\
            p$_{\text{3D}}$     & Pressure on model levels          & mb                                 & 3D  \\
            z                   & Layer altitude (above MSL)        & m                                  & 3D  \\
            t                   & Temperature                       & K                                  & 3D  \\
            th                  & Potential temperature             & K                                  & 3D  \\
            q                   & Specific humidity                 & kg/kg                              & 3D  \\
            prec$_{\text{3D}}$  & Precipitation                     & mm/d                               & 3D  \\
            cfrad               & Cloud fraction                    & \%                                 & 3D  \\
            mcuflx              & Convective updraft mass flux      & kg m$^{-2}$ s$^{-1}$               & 3D  \\
            airmass             & Air mass density                  & kg m$^{-2}$                        & 3D  \\
            BCB                 & BCB mixing ratio                  & $10^{-11}$ kg/kg$_{\text{air}}$    & 3D  \\ \hline
        \end{tabular}
    \end{table}
    
    accoridng to equaiton 1 and depending on how to parametrtize the convetion term, the list of required variables would be varried. out of the mentioned variables few of those were excluded based on some preliminary analysis such as Estimate mutual information for a continuous target variable, Univariate linear regression tests, recursive feature elimination and feature importance of Random forest. this preliminary analysis was nececarry as we learn in the training procedures, loading all of the mentioned data in 2D and 3D format will make a great bottleneck in the process of traininig. eventually we conclude with 2 static variabels represeting the era of geach gridcell and the fraction of land, sea and ice, and 4 2D variables which are exting on the sufrace pressure including  Planetary boundary layer, Sensible heat flux, Latent heat flux and BCB biomass source. 3D variables include velocity field in all three direction, pressure tempretaure, potential temperature and precipitaiton.
    
    \begin{equation}
        \frac{du}{dt} + u\frac{du}{dx} + v\frac{du}{dy} + w(\omega,T,q,z)\frac{du}{dz} = \Sigma
    \end{equation}

    The scope of this project falls into troposphere Based on the the range of pressures for troposphere between 1000 to 200 mbar, the number of total levels should be considered in this study to cover the whole troposphere is between 30 to 40. However, considering the BCB concentration distribution on the vertical levels, the BCB concentration drop significantly after level 20th. this makes data preprocessing and training very laborious. given the fact that ML architecture with high inductive biases on spatial domain generally focus on the regions with high concentration to avoid penalization through loss function (Mean Square Error in this case). eventually the first 20 vertical levels (up to 600 hPa) were considered to be processed. This vertical range covers nearly the entire BCB concentration distribution in the troposphere.

\section{Model Architecture}
    The model receives 2D and 3D data separately. We used spatial encoders to process two different modalities (emission and forcing). Suppose a batch of 3D input tensors $\mathbf{B} \in \mathbb{R}^{B \times T \times V \times L \times H \times W}$, where the batch size is $B = |\mathbf{B}|$, $V$ represents the number of 3D variables, and $L$ represents the number of associated levels for each variable. In the spatial encoder, we reshape the sequential input data from $B \times T \times V \times L \times H \times W$ to $(B \times T) \times (V \times L) \times H \times W$ so that only spatial correlations are taken into account. The same procedure is applied to 2D tensors with the number of levels equal to 1. The outputs of the 3D and 2D encoders are $(B \times T) \times F_1 \times H \times W$ and $(B \times T) \times F_2 \times H \times W$, respectively, where $F_1 = F_2$ is the number of channels in the latent space. The outputs of the encoders are fused via Spatial Feature Transform (SFT). In fact, SFT modulates 3D features conditioned on 2D features (probability maps). In other words, the SFT layer generates affine transformation parameters for spatial-wise feature modulation. SFT layers can be trained end-to-end together with the network using the same loss function. This module is designed for spatial feature extraction and modality fusion. The output of this step is in the shape of $B \times T \times F_m \times H \times W$, which can be fed into any spatio-temporal ML model to impose the effect of time evolution on the latent features and map them to the target variable.

\section{Experimental Settings}
    The training consists of two tiers. First, the model is trained on data from 1851--1852, comprising 35,040 timesteps, for 50 epochs. The training uses the Adam optimizer with an initial learning rate of 0.0005 and a Cosine Annealing scheduler. To avoid unstable training, the first 10 steps are considered as a warm-up period with a factor of 0.1 and Linear scheduler. In the second tier, the model is then fine-tuned on data from 2010--2011 for 30 epochs using the Adam optimizer with an initial learning rate of $1/5$ of the first tier (i.e., 0.0001) and a Cosine Annealing scheduler. For each round of training, ten percent of the training period is randomly selected as the validation set. However, the entire years of 1853, 2012, 2020, and 2021 are reserved for testing. Therefore, the model is trained on 4 years of data and tested on 4 years, where 2 test years (2020 and 2021) represent entirely different time periods outside the training domain. This configuration provides a crucial capability for climate modeling applications requiring reliable long-term projections, as it evaluates the model's ability to generalize to future climate conditions not seen during training.

\section{Results}
    



    



\bibliographystyle{unsrt}
\bibliography{ref.bib}

\end{document}